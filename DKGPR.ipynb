{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DKGPR\n",
    "\n",
    "This notebook demonstrates how to use the DKGPR for:\n",
    "1. Basic regression\n",
    "2. Uncertainty quantification\n",
    "3. Confidence weighting\n",
    "4. Bayesian optimization\n",
    "\n",
    "**Author:** Yongtao Liu \n",
    "\n",
    "**Date:** January 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, make sure the package is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10fc45e90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yla/ORNL Dropbox/Yongtao Liu/MyFiles/CNMS_2025/Githublocal/DKGP/.venv/lib/python3.14/site-packages/pyro/ops/stats.py:527: SyntaxWarning: \"\\g\" is an invalid escape sequence. Such sequences will not work in the future. Did you mean \"\\\\g\"? A raw string is also an option.\n",
      "  we have :math:`ES^{*}(P,Q) \\ge ES^{*}(Q,Q)` with equality holding if and only if :math:`P=Q`, i.e.\n",
      "/Users/yla/ORNL Dropbox/Yongtao Liu/MyFiles/CNMS_2025/Githublocal/DKGP/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Deep Kernel GP imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import DKGP\n",
    "from dkgp import fit_dkgp, predict\n",
    "from dkgp.acquisition import (\n",
    "    expected_improvement,\n",
    "    upper_confidence_bound,\n",
    "    probability_of_improvement\n",
    ")\n",
    "from dkgp.prediction import predict_with_epistemic_aleatoric\n",
    "from dkgp.utils import (\n",
    "    standardize_data,\n",
    "    compute_metrics,\n",
    "    compute_calibration,\n",
    "    print_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Regression\n",
    "\n",
    "Let's start with a simple regression problem on high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (200, 100)\n",
      "Test set: (50, 100)\n",
      "Target range: [-5.93, 7.06]\n"
     ]
    }
   ],
   "source": [
    "def generate_data(n_samples=100, input_dim=50, noise=0.1):\n",
    "    \"\"\"\n",
    "    Generate synthetic high-dimensional data.\n",
    "    True function: non-linear combination of first few features\n",
    "    \"\"\"\n",
    "    X = np.random.randn(n_samples, input_dim)\n",
    "    \n",
    "    # True underlying function (only first 5 features matter)\n",
    "    y = (X[:, 0] + \n",
    "         2 * X[:, 1] - \n",
    "         X[:, 2] + \n",
    "         0.5 * X[:, 3]**2 + \n",
    "         np.sin(X[:, 4]) + \n",
    "         noise * np.random.randn(n_samples))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate training data\n",
    "X_train, y_train = generate_data(n_samples=200, input_dim=100, noise=0.1)\n",
    "\n",
    "# Generate test data\n",
    "X_test, y_test = generate_data(n_samples=50, input_dim=100, noise=0.1)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Target range: [{y_train.min():.2f}, {y_train.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training Deep Kernel GP Regression Model\n",
      "============================================================\n",
      "\n",
      "Auto-selected: ExactMarginalLogLikelihood\n",
      "\n",
      "Training Deep Kernel GP\n",
      "============================================================\n",
      "  Device: cpu\n",
      "  Input dim: 100 â†’ Feature dim: 16\n",
      "  Data points: 200\n",
      "  Hidden layers: [256, 128, 64]\n",
      "  MLL: ExactMarginalLogLikelihood\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train Deep Kernel GP\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m mll, gp_model, dkl_model, losses = \u001b[43mfit_dkgp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Reduce 100D -> 16D\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Neural network architecture\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# Training iterations\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_features\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Learning rate for neural network\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_gp\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Learning rate for GP\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplot_loss\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# We'll plot manually\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ORNL Dropbox/Yongtao Liu/MyFiles/CNMS_2025/Githublocal/DKGP/.venv/lib/python3.14/site-packages/dkgp/training.py:293\u001b[39m, in \u001b[36mfit_dkgp\u001b[39m\u001b[34m(X_train, y_train, confidence_weights, use_custom_mll, feature_dim, hidden_dims, num_epochs, lr_features, lr_gp, device, verbose, plot_loss, patience, min_delta)\u001b[39m\n\u001b[32m    289\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m    291\u001b[39m input_dim = X_train.shape[-\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m dkl_model, losses = \u001b[43mtrain_dkgp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatapoints\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfidence_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfidence_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_custom_mll\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_custom_mll\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_gp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr_gp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_delta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_delta\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m gp_model = dkl_model.gp_model\n\u001b[32m    312\u001b[39m \u001b[38;5;66;03m# Create MLL for reference\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ORNL Dropbox/Yongtao Liu/MyFiles/CNMS_2025/Githublocal/DKGP/.venv/lib/python3.14/site-packages/dkgp/training.py:186\u001b[39m, in \u001b[36mtrain_dkgp\u001b[39m\u001b[34m(datapoints, targets, input_dim, feature_dim, hidden_dims, confidence_weights, use_custom_mll, num_epochs, lr_features, lr_gp, device, verbose, patience, min_delta)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[32m    184\u001b[39m loss = -mll(output, model.gp_model.train_targets)\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m    188\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ORNL Dropbox/Yongtao Liu/MyFiles/CNMS_2025/Githublocal/DKGP/.venv/lib/python3.14/site-packages/torch/_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ORNL Dropbox/Yongtao Liu/MyFiles/CNMS_2025/Githublocal/DKGP/.venv/lib/python3.14/site-packages/torch/autograd/__init__.py:357\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    354\u001b[39m     tensors = \u001b[38;5;28mtuple\u001b[39m(tensors)\n\u001b[32m    356\u001b[39m grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m grad_tensors_ = \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ORNL Dropbox/Yongtao Liu/MyFiles/CNMS_2025/Githublocal/DKGP/.venv/lib/python3.14/site-packages/torch/autograd/__init__.py:205\u001b[39m, in \u001b[36m_make_grads\u001b[39m\u001b[34m(outputs, grads, is_grads_batched)\u001b[39m\n\u001b[32m    203\u001b[39m     out_numel_is_1 = out.numel() == \u001b[32m1\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_numel_is_1:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    206\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m     )\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out_dtype.is_floating_point:\n\u001b[32m    209\u001b[39m     msg = (\n\u001b[32m    210\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    211\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    212\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "# Train Deep Kernel GP\n",
    "mll, gp_model, dkl_model, losses = fit_dkgp(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    feature_dim=16,           # Reduce 100D -> 16D\n",
    "    hidden_dims=[256, 128, 64],  # Neural network architecture\n",
    "    num_epochs=1000,          # Training iterations\n",
    "    lr_features=1e-4,         # Learning rate for neural network\n",
    "    lr_gp=1e-2,              # Learning rate for GP\n",
    "    verbose=True,\n",
    "    plot_loss=False          # We'll plot manually\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses, linewidth=2, color='#2E86AB')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Negative Log Likelihood', fontsize=12)\n",
    "plt.title('Training Loss', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(losses[100:], linewidth=2, color='#A23B72')  # Skip first 100 epochs\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Negative Log Likelihood', fontsize=12)\n",
    "plt.title('Training Loss (after epoch 100)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "mean_pred, std_pred = predict(dkl_model, X_test, return_std=True)\n",
    "\n",
    "print(f\"Predictions shape: {mean_pred.shape}\")\n",
    "print(f\"Uncertainty shape: {std_pred.shape}\")\n",
    "print(f\"Mean uncertainty: {std_pred.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "metrics = compute_metrics(y_test, mean_pred, std_pred)\n",
    "print_metrics(metrics, \"Test Set Performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 1. Predictions vs True\n",
    "axes[0].scatter(y_test, mean_pred, alpha=0.6, s=80, edgecolors='k', linewidth=1)\n",
    "min_val = min(y_test.min(), mean_pred.min())\n",
    "max_val = max(y_test.max(), mean_pred.max())\n",
    "axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect')\n",
    "axes[0].set_xlabel('True Values', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Values', fontsize=12)\n",
    "axes[0].set_title(f'Predictions (RÂ²={metrics[\"r2\"]:.3f})', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Residuals\n",
    "residuals = y_test - mean_pred\n",
    "axes[1].scatter(mean_pred, residuals, alpha=0.6, s=80, edgecolors='k', linewidth=1)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Values', fontsize=12)\n",
    "axes[1].set_ylabel('Residuals', fontsize=12)\n",
    "axes[1].set_title('Residual Plot', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Predictions with uncertainty\n",
    "sorted_idx = np.argsort(y_test)\n",
    "x_axis = np.arange(len(y_test))\n",
    "\n",
    "axes[2].plot(x_axis, y_test[sorted_idx], 'o-', label='True', linewidth=2, markersize=7)\n",
    "axes[2].plot(x_axis, mean_pred[sorted_idx], 's-', label='Predicted', linewidth=2, markersize=7)\n",
    "axes[2].fill_between(\n",
    "    x_axis,\n",
    "    mean_pred[sorted_idx] - 2*std_pred[sorted_idx],\n",
    "    mean_pred[sorted_idx] + 2*std_pred[sorted_idx],\n",
    "    alpha=0.3,\n",
    "    label='95% CI'\n",
    ")\n",
    "axes[2].set_xlabel('Test Sample (sorted)', fontsize=12)\n",
    "axes[2].set_ylabel('Value', fontsize=12)\n",
    "axes[2].set_title('Predictions with Uncertainty', fontsize=13, fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2ï¸âƒ£ Uncertainty Quantification\n",
    "\n",
    "Deep Kernel GP provides two types of uncertainty:\n",
    "- **Epistemic**: Model uncertainty (reducible with more data)\n",
    "- **Aleatoric**: Observation noise (irreducible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose uncertainty\n",
    "mean_unc, epistemic, aleatoric, total = predict_with_epistemic_aleatoric(\n",
    "    dkl_model, X_test\n",
    ")\n",
    "\n",
    "# Convert to std (from variance)\n",
    "epistemic_std = np.sqrt(epistemic)\n",
    "aleatoric_std = np.sqrt(aleatoric)\n",
    "total_std = np.sqrt(total)\n",
    "\n",
    "print(\"Uncertainty Decomposition:\")\n",
    "print(f\"  Epistemic (model):      {epistemic_std.mean():.4f} Â± {epistemic_std.std():.4f}\")\n",
    "print(f\"  Aleatoric (noise):      {aleatoric_std.mean():.4f} Â± {aleatoric_std.std():.4f}\")\n",
    "print(f\"  Total:                  {total_std.mean():.4f} Â± {total_std.std():.4f}\")\n",
    "print(f\"  Epistemic/Aleatoric:    {epistemic_std.mean()/aleatoric_std.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize uncertainty decomposition\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Stacked bar chart\n",
    "x_axis = np.arange(len(y_test))\n",
    "axes[0].bar(x_axis, epistemic_std, label='Epistemic', alpha=0.8, color='#F18F01')\n",
    "axes[0].bar(x_axis, aleatoric_std, bottom=epistemic_std, label='Aleatoric', alpha=0.8, color='#C73E1D')\n",
    "axes[0].set_xlabel('Test Sample', fontsize=12)\n",
    "axes[0].set_ylabel('Uncertainty (std)', fontsize=12)\n",
    "axes[0].set_title('Uncertainty Decomposition', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Scatter plot\n",
    "axes[1].scatter(epistemic_std, aleatoric_std, alpha=0.6, s=80, edgecolors='k', linewidth=1)\n",
    "axes[1].set_xlabel('Epistemic Uncertainty', fontsize=12)\n",
    "axes[1].set_ylabel('Aleatoric Uncertainty', fontsize=12)\n",
    "axes[1].set_title('Epistemic vs Aleatoric', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Analysis\n",
    "\n",
    "Check if our uncertainty estimates are well-calibrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute calibration\n",
    "calibration = compute_calibration(y_test, mean_pred, std_pred, n_bins=10)\n",
    "\n",
    "print(f\"Mean Absolute Calibration Error: {calibration['mace']:.4f}\")\n",
    "print(\"(Lower is better, 0 = perfect calibration)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot calibration curve\n",
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "conf = calibration['confidence_levels']\n",
    "obs = calibration['observed_coverage']\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Perfect Calibration')\n",
    "plt.plot(conf, obs, 'o-', linewidth=3, markersize=10, color='#2E86AB', label='Model')\n",
    "\n",
    "plt.xlabel('Expected Coverage', fontsize=13)\n",
    "plt.ylabel('Observed Coverage', fontsize=13)\n",
    "plt.title(f'Calibration Curve\\n(MACE = {calibration[\"mace\"]:.4f})', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3ï¸âƒ£ Confidence Weighting\n",
    "\n",
    "Sometimes data points have different reliability. We can weight them accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data with Varying Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with heteroscedastic noise\n",
    "n_train = 150\n",
    "X_noisy = np.random.randn(n_train, 50)\n",
    "\n",
    "# True function\n",
    "y_true = X_noisy[:, 0] + 2*X_noisy[:, 1] + np.sin(X_noisy[:, 2])\n",
    "\n",
    "# Noise level varies with input\n",
    "noise_level = 0.1 + 0.5 * np.abs(X_noisy[:, 0])  # Higher noise for larger |x[0]|\n",
    "y_noisy = y_true + noise_level * np.random.randn(n_train)\n",
    "\n",
    "print(f\"Noise level range: [{noise_level.min():.3f}, {noise_level.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize noise distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_noisy[:, 0], y_noisy, c=noise_level, s=50, cmap='YlOrRd', edgecolors='k', linewidth=0.5)\n",
    "plt.colorbar(label='Noise Level')\n",
    "plt.xlabel('X[0]', fontsize=12)\n",
    "plt.ylabel('Y (with noise)', fontsize=12)\n",
    "plt.title('Data with Varying Noise', fontsize=13, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(noise_level, bins=20, edgecolor='k', alpha=0.7, color='#A23B72')\n",
    "plt.xlabel('Noise Level', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Noise Distribution', fontsize=13, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with Confidence Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confidence weights (inverse of noise level)\n",
    "# Lower noise -> Higher confidence\n",
    "confidence = 1.0 / (1.0 + noise_level**2)\n",
    "\n",
    "print(f\"Confidence range: [{confidence.min():.3f}, {confidence.max():.3f}]\")\n",
    "print(f\"High noise -> Low confidence: {confidence[noise_level > 0.4].mean():.3f}\")\n",
    "print(f\"Low noise -> High confidence: {confidence[noise_level < 0.2].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train WITHOUT confidence weights\n",
    "print(\"Training WITHOUT confidence weighting...\")\n",
    "_, _, model_no_weight, _ = fit_dkgp(\n",
    "    X_noisy, y_noisy,\n",
    "    feature_dim=16,\n",
    "    num_epochs=800,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Train WITH confidence weights\n",
    "print(\"Training WITH confidence weighting...\")\n",
    "_, _, model_weighted, _ = fit_dkgp(\n",
    "    X_noisy, y_noisy,\n",
    "    confidence_weights=confidence,\n",
    "    feature_dim=16,\n",
    "    num_epochs=800,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"âœ… Both models trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate clean test data\n",
    "X_test_clean = np.random.randn(50, 50)\n",
    "y_test_clean = X_test_clean[:, 0] + 2*X_test_clean[:, 1] + np.sin(X_test_clean[:, 2])\n",
    "\n",
    "# Predictions\n",
    "pred_no_weight, _ = predict(model_no_weight, X_test_clean)\n",
    "pred_weighted, _ = predict(model_weighted, X_test_clean)\n",
    "\n",
    "# Compute errors\n",
    "mse_no_weight = np.mean((y_test_clean - pred_no_weight)**2)\n",
    "mse_weighted = np.mean((y_test_clean - pred_weighted)**2)\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(f\"  Without weighting: MSE = {mse_no_weight:.4f}\")\n",
    "print(f\"  With weighting:    MSE = {mse_weighted:.4f}\")\n",
    "print(f\"  Improvement:       {(1 - mse_weighted/mse_no_weight)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test_clean, pred_no_weight, alpha=0.6, s=80, label='Without Weighting')\n",
    "plt.scatter(y_test_clean, pred_weighted, alpha=0.6, s=80, label='With Weighting')\n",
    "min_val = y_test_clean.min()\n",
    "max_val = y_test_clean.max()\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect')\n",
    "plt.xlabel('True Values', fontsize=12)\n",
    "plt.ylabel('Predicted Values', fontsize=12)\n",
    "plt.title('Predictions: Weighted vs Unweighted', fontsize=13, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "errors_no_weight = np.abs(y_test_clean - pred_no_weight)\n",
    "errors_weighted = np.abs(y_test_clean - pred_weighted)\n",
    "x = np.arange(len(y_test_clean))\n",
    "width = 0.35\n",
    "plt.bar(x - width/2, errors_no_weight, width, label='Without Weighting', alpha=0.8)\n",
    "plt.bar(x + width/2, errors_weighted, width, label='With Weighting', alpha=0.8)\n",
    "plt.xlabel('Test Sample', fontsize=12)\n",
    "plt.ylabel('Absolute Error', fontsize=12)\n",
    "plt.title('Error Comparison', fontsize=13, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4ï¸âƒ£ Bayesian Optimization\n",
    "\n",
    "Use Deep Kernel GP to optimize an expensive black-box function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def branin_high_dim(x):\n",
    "    \"\"\"\n",
    "    Branin function (classic optimization benchmark) in high dimensions.\n",
    "    Only first 2 dimensions matter, rest are noise.\n",
    "    Global minimum â‰ˆ 0.398 at (Ï€, 2.275) and two other locations.\n",
    "    \"\"\"\n",
    "    x1 = x[0] * 15 - 5   # Scale to [-5, 10]\n",
    "    x2 = x[1] * 15        # Scale to [0, 15]\n",
    "    \n",
    "    a = 1\n",
    "    b = 5.1 / (4 * np.pi**2)\n",
    "    c = 5 / np.pi\n",
    "    r = 6\n",
    "    s = 10\n",
    "    t = 1 / (8 * np.pi)\n",
    "    \n",
    "    term1 = a * (x2 - b * x1**2 + c * x1 - r)**2\n",
    "    term2 = s * (1 - t) * np.cos(x1)\n",
    "    term3 = s\n",
    "    \n",
    "    # Add small noise from other dimensions\n",
    "    if len(x) > 2:\n",
    "        noise = 0.05 * np.sum(x[2:]**2)\n",
    "    else:\n",
    "        noise = 0\n",
    "    \n",
    "    return term1 + term2 + term3 + noise\n",
    "\n",
    "print(f\"True global minimum: â‰ˆ 0.398\")\n",
    "print(f\"Test evaluation: {branin_high_dim(np.array([0.5424, 0.1517] + [0]*18)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize with Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem setup\n",
    "input_dim = 20\n",
    "n_initial = 10\n",
    "n_iterations = 15\n",
    "\n",
    "# Generate candidate pool\n",
    "n_candidates = 2000\n",
    "candidates = np.random.uniform(0, 1, size=(n_candidates, input_dim))\n",
    "\n",
    "# Initial random samples\n",
    "initial_idx = np.random.choice(n_candidates, n_initial, replace=False)\n",
    "X_observed = candidates[initial_idx]\n",
    "y_observed = np.array([branin_high_dim(x) for x in X_observed])\n",
    "\n",
    "print(f\"Initial samples: {n_initial}\")\n",
    "print(f\"Initial best: {y_observed.min():.4f}\")\n",
    "print(f\"Candidate pool: {n_candidates} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Bayesian Optimization Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track progress\n",
    "best_values = [y_observed.min()]\n",
    "all_values = list(y_observed)\n",
    "ei_values_history = []\n",
    "\n",
    "print(\"Starting Bayesian Optimization...\\n\")\n",
    "print(f\"{'Iter':<6} {'Next f(x)':<12} {'Best f(x)':<12} {'Max EI':<12}\")\n",
    "print(\"=\"*48)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    # Train model on current observations\n",
    "    _, _, model, _ = fit_dkgp(\n",
    "        X_observed,\n",
    "        y_observed,\n",
    "        feature_dim=16,\n",
    "        num_epochs=500,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Get current best\n",
    "    best_f = y_observed.min()\n",
    "    \n",
    "    # Remove already evaluated candidates\n",
    "    mask = np.ones(len(candidates), dtype=bool)\n",
    "    for x in X_observed:\n",
    "        mask &= ~np.all(np.isclose(candidates, x), axis=1)\n",
    "    available = candidates[mask]\n",
    "    \n",
    "    # Compute Expected Improvement\n",
    "    ei = expected_improvement(\n",
    "        model,\n",
    "        available,\n",
    "        best_f=best_f,\n",
    "        xi=0.01,\n",
    "        maximize=False  # Minimize Branin\n",
    "    )\n",
    "    \n",
    "    # Select next point\n",
    "    next_idx = np.argmax(ei)\n",
    "    next_point = available[next_idx]\n",
    "    next_value = branin_high_dim(next_point)\n",
    "    \n",
    "    # Update observations\n",
    "    X_observed = np.vstack([X_observed, next_point])\n",
    "    y_observed = np.append(y_observed, next_value)\n",
    "    \n",
    "    # Track progress\n",
    "    best_values.append(y_observed.min())\n",
    "    all_values.append(next_value)\n",
    "    ei_values_history.append(ei.max())\n",
    "    \n",
    "    print(f\"{iteration+1:<6} {next_value:<12.4f} {y_observed.min():<12.4f} {ei.max():<12.6f}\")\n",
    "\n",
    "print(\"=\"*48)\n",
    "print(f\"\\nOptimization complete!\")\n",
    "print(f\"Final best value: {y_observed.min():.4f}\")\n",
    "print(f\"True global minimum: 0.398\")\n",
    "print(f\"Gap: {y_observed.min() - 0.398:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Optimization Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Best value over iterations\n",
    "axes[0, 0].plot(best_values, 'o-', linewidth=2, markersize=8, color='#2E86AB')\n",
    "axes[0, 0].axhline(y=0.398, color='r', linestyle='--', linewidth=2, label='Global minimum')\n",
    "axes[0, 0].set_xlabel('Iteration', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Best f(x) Found', fontsize=12)\n",
    "axes[0, 0].set_title('Optimization Progress', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. All evaluations\n",
    "axes[0, 1].plot(all_values, 'o', alpha=0.6, markersize=6, color='#A23B72')\n",
    "axes[0, 1].axhline(y=0.398, color='r', linestyle='--', linewidth=2, label='Global minimum')\n",
    "axes[0, 1].axvline(x=n_initial-0.5, color='k', linestyle=':', linewidth=2, label='Initial samples')\n",
    "axes[0, 1].set_xlabel('Evaluation', fontsize=12)\n",
    "axes[0, 1].set_ylabel('f(x)', fontsize=12)\n",
    "axes[0, 1].set_title('All Evaluations', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Expected Improvement over iterations\n",
    "axes[1, 0].plot(ei_values_history, 's-', linewidth=2, markersize=7, color='#F18F01')\n",
    "axes[1, 0].set_xlabel('Iteration', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Max Expected Improvement', fontsize=12)\n",
    "axes[1, 0].set_title('Acquisition Function Values', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Cumulative improvement\n",
    "improvements = np.maximum.accumulate(-np.array(best_values))  # Negative for minimization\n",
    "axes[1, 1].plot(improvements, 'o-', linewidth=2, markersize=8, color='#C73E1D')\n",
    "axes[1, 1].set_xlabel('Iteration', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Cumulative Improvement', fontsize=12)\n",
    "axes[1, 1].set_title('Total Improvement Over Initial Best', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Acquisition Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model\n",
    "_, _, final_model, _ = fit_dkgp(\n",
    "    X_observed, y_observed,\n",
    "    feature_dim=16,\n",
    "    num_epochs=500,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Sample some test candidates\n",
    "test_candidates = np.random.uniform(0, 1, (100, input_dim))\n",
    "best_f = y_observed.min()\n",
    "\n",
    "# Compute different acquisition functions\n",
    "ei = expected_improvement(final_model, test_candidates, best_f, maximize=False)\n",
    "ucb = upper_confidence_bound(final_model, test_candidates, beta=2.0, maximize=False)\n",
    "pi = probability_of_improvement(final_model, test_candidates, best_f, maximize=False)\n",
    "\n",
    "# Normalize for comparison\n",
    "ei_norm = (ei - ei.min()) / (ei.max() - ei.min() + 1e-8)\n",
    "ucb_norm = (ucb - ucb.min()) / (ucb.max() - ucb.min() + 1e-8)\n",
    "pi_norm = (pi - pi.min()) / (pi.max() - pi.min() + 1e-8)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "x = np.arange(len(test_candidates))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar(x - width, ei_norm, width, label='EI', alpha=0.8)\n",
    "plt.bar(x, ucb_norm, width, label='UCB', alpha=0.8)\n",
    "plt.bar(x + width, pi_norm, width, label='PI', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Candidate Index', fontsize=12)\n",
    "plt.ylabel('Normalized Acquisition Value', fontsize=12)\n",
    "plt.title('Comparison of Acquisition Functions', fontsize=13, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show top candidates for each\n",
    "print(\"\\nTop 3 candidates by acquisition function:\")\n",
    "print(f\"  EI:  indices {np.argsort(ei)[-3:][::-1]}\")\n",
    "print(f\"  UCB: indices {np.argsort(ucb)[:3]}\")\n",
    "print(f\"  PI:  indices {np.argsort(pi)[-3:][::-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“š Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "1. âœ… **Basic Regression**: Training on high-dimensional data\n",
    "2. âœ… **Uncertainty Quantification**: Epistemic vs aleatoric uncertainty\n",
    "3. âœ… **Confidence Weighting**: Handling varying data quality\n",
    "4. âœ… **Bayesian Optimization**: Using acquisition functions\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Deep Kernel GP combines neural networks with GPs for high-dimensional regression\n",
    "- Provides well-calibrated uncertainty estimates\n",
    "- Confidence weighting improves performance on noisy data\n",
    "- Multiple acquisition functions available for Bayesian optimization\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try on your own dataset\n",
    "- Experiment with different architectures (`feature_dim`, `hidden_dims`)\n",
    "- Compare acquisition functions on your optimization problem\n",
    "- Save/load models with `utils.save_model()` and `utils.load_model()`\n",
    "\n",
    "Happy modeling! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DKGP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
